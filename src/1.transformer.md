# Chapter 1: Attention & Transformers

## Attention & Self-Attention

人类在处理信息时会“有选择地关注”最相关的部分，比如看一张图时注意某个局部，读文章时注意关键句子。

基本思想：给输入信息的不同部分分配不同的权重，然后对加权后的信息进行整合。

最常见的是**加性注意力或点积注意力**，核心是
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

- $Q$: 当前需要处理的对象
- $K$: 输入序列中的所有元素的键
- $V$: 输入序列中的所有元素的值
- $softmax(QK^{T})$: 表示“Query 与 Key 的相似度”，越相关权重越大

Self-Attention 实际上是 Attention 机制的一种特殊情况，特点是 **Q、K、V 都来自同一个序列**。

作用：序列中的每个元素都能和序列中的所有其他元素计算相关性，从而获取全局上下文信息。

$$\text{SelfAttention}(X) = text{softmax}\left(\frac{XW_Q(XW_K)^T}{\sqrt{d_k}}\right)XW_V$$
$X$ 来自于同一个序列

## Why Transformer use Self-Attention?

### 传统的序列模型处理方式

在传统的序列处理模型（如 RNN、LSTM 和 GRU）中，模型是按顺序逐个处理序列中的元素（例如单词或字符），并且每个元素的处理依赖于前一个元素的隐藏状态。

> :warning: 这种方法在处理长序列时会面临**梯度消失或梯度爆炸的问题**，导致模型难以捕捉长距离的依赖关系。

- Transformer 的计算方式对 GPU 更友好，能更好地利用并行计算资源
  - RNN 需要按时间步顺序处理，难以并行化
  - Transformer 的 Self-Attention 机制允许同时处理序列中的所有位置，充分利用 GPU 的并行计算能力

## Transformers 结构

### 输入 X 与 attention_mask 的 shape

- 输入 X 一般形状为`[batch_size, seq_len, embedding_dim]`
- attention_mask 是经过 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略形状一般是`[batch_size, num_heads, seq_len]`

### 为什么用 Transformer 中用 layer norm

| 特性             | Batch Norm         | Layer Norm             | RMSNorm                                            |
| ---------------- | ------------------ | ---------------------- | -------------------------------------------------- |
| 标准化维度       | 小批量内各特征维度 | 每个样本的所有特征维度 | 每个样本的特征维度的均方根                         |
| 计算开销         | 中等               | 较大                   | 较小                                               |
| 对小批量大小依赖 | 依赖               | 不依赖                 | 不依赖                                             |
| 应用场景         | CNN、MLP           | RNN、Transformer       | 各类神经网络，尤其在计算效率和稳定性要求高的任务中 |
| 正则化效果       | 有一定正则化效果   | 无显著正则化效果       | 无显著正则化效果                                   |

1. 列长度的灵活性：
   Transformer 处理的是序列数据，序列长度可能因输入样本而异。LayerNorm 对每个样本自身的一层神经元的输入进行归一化，与其他样本的序列长度无关，**能够很好地处理不同长度的输入序列**。而 batch norm 对长度大小不同的 NLP 任务计算的超参数泛化能力差。
2. 并行计算的适应性：
   Transformer 的多头注意力机制高度并行化，LayerNorm 只需要**对单个样本的一层进行计算，不需要等待其他样本的信息，因此更适合并行计算环境**。
3. 模型的稳定性：
   LayerNorm 基于每一层自身的输入进行归一化，能够更好地控制每一层输入的范围和分布，避免梯度消失或梯度爆炸问题。

### post-norm 与 pre-norm

- 原始的 transformer 中使用的是 post-norm，而 llm 中大多使用 pre-norm

| norm 位置 | 优点                                                                                                                                                                                                                                                    | 缺点                                                                                                                                                                                     |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| pre-norm  | **训练稳定**：在残差连接之前进行归一化，可以有效缓解梯度消失或爆炸的问题，使深层网络的训练更加稳定。**收敛速度快**：梯度能够更直接地传递到前面的层，从而加快模型的整体收敛速度。**减少调参工作**：不需要像 Post-Norm 那样依赖复杂的学习率预热等优化技巧 | 潜在的表示塌陷问题：靠近输出位置的层可能会变得非常相似，从而对模型的贡献变小，限制了模型的上限。可能削弱层的贡献：由于先进行了归一化，可能会减弱每一层的实际贡献，导致模型的有效深度变浅 |
| post-norm | **保留输入特征**：更接近原始输入的特征，有助于信息的传递。**潜在性能优势**：虽然训练不稳定，但有研究暗示其在效果上可能有更大的潜力                                                                                                                      | 训练不稳定：在深层模型中，梯度容易爆炸或消失，对学习率和权重初始化非常敏感，收敛困难。依赖优化技巧：需要使用学习率预热等复杂的优化方法来稳定训练                                         |
